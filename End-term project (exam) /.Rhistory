#### Empirical bootstrap algorithm
B <- 1000 # number of bootstrap runs
set.seed(1000)
Vt_boot <- t(sapply(1:B, function(b){
### draw samples from the observed data; using empirical bootstrapping
data_boot <- data[sample(1:length(data$ID), replace = TRUE),]
x1_boot <- data_boot[,1]; x2_boot <- data_boot[,2]
### using the above-mentioned estimate function to get estimated parameters
est_boot <- estimate(x1_boot,x2_boot)
### create est2 for importance sampling
est2_boot <- est_boot
est2_boot$est1[1] <- est2_boot$est1[1]+1
est2_boot$est2[1] <- est2_boot$est1[1]+0.1
V3 <- c()
### 1000 bootstrapping simulations using importance sampling
for (i in 1:10^5){
Y <- rmycopula(length(data$ID), est2$est1[1], est2$est1[2],
est2$est2[1], est2$est2[2], est2$est3)
total_claim <- rowSums(Y)
density_div <- dmycopula(Y, est) / dmycopula(Y, est2)
V_new <- sapply(seq(100,200,10), function(t) sum(total_claim[total_claim > t]*
density_div[total_claim > t]))
V3 <- rbind(V3,V_new)
}
return(t(colMeans(V3)))
}))
rm(simulated_data)
#### Empirical bootstrap algorithm
B <- 2#1000 # number of bootstrap runs
set.seed(1000)
system.time(
Vt_boot <- t(sapply(1:B, function(b){
### draw samples from the observed data; using empirical bootstrapping
data_boot <- data[sample(1:length(data$ID), replace = TRUE),]
x1_boot <- data_boot[,1]; x2_boot <- data_boot[,2]
### using the above-mentioned estimate function to get estimated parameters
est_boot <- estimate(x1_boot,x2_boot)
### create est2 for importance sampling
est2_boot <- est_boot
est2_boot$est1[1] <- est2_boot$est1[1]+1
est2_boot$est2[1] <- est2_boot$est1[1]+0.1
V3 <- c()
### 1000 bootstrapping simulations using importance sampling
for (i in 1:10^5){
Y <- rmycopula(length(data$ID), est2$est1[1], est2$est1[2],
est2$est2[1], est2$est2[2], est2$est3)
total_claim <- rowSums(Y)
density_div <- dmycopula(Y, est) / dmycopula(Y, est2)
V_new <- sapply(seq(100,200,10), function(t) sum(total_claim[total_claim > t]*
density_div[total_claim > t]))
V3 <- rbind(V3,V_new)
}
return(t(colMeans(V3)))
})))
3482/60
View(Vt_boot)
k
knitr::opts_chunk$set(cache=TRUE,
message=FALSE, warning=FALSE,
fig.path='figs/',
cache.path = '_cache/',
fig.process = function(x) {
x2 = sub('-\\d+([.][a-z]+)$', '\\1', x)
if (file.rename(x, x2)) x2 else x
})
library(copula)
library(WVPlots)
library(gridExtra)
library(tidyverse)
library(stargazer)
options(digits.secs = 3)
load('fit_n_times.RData')
### plot time
plot(time, type ='b')
### plot RMSE
RMSE <- as.data.frame(RMSE)
names(RMSE) <- c('mu1', 'sigma1', 'mu2', 'sigma2', 'theta')
plot(NULL, xlab = 'number_of_runs', ylab = 'RMSE',
xlim=c(0, 10^3), ylim=c(min(RMSE), max(RMSE)))
number_of_runs <- c(2,5,10) * 100
lines(number_of_runs, type ='b', RMSE$mu1, col='red')
lines(number_of_runs, type ='b', RMSE$sigma1, col='orange')
lines(number_of_runs, type ='b', RMSE$mu2, col='black')
lines(number_of_runs, type ='b', RMSE$sigma2, col='blue')
lines(number_of_runs, type ='b', RMSE$theta, col='green')
legend( x = 'topright', legend = names(RMSE), pch=1,
col = c('red', 'orange', 'black', 'blue', 'green'))
B <- 10^5 # number of simulation runs
### simulated B data points to save time
simulated_data <- rmycopula(B, est$est1[1], est$est1[2], est$est2[1], est$est2[2], est$est3)
total_claim <- rowSums(simulated_data) # x1 + x2
total_claim <- matrix(total_claim, nrow=k)
V <- sapply(seq(100,200,10), function(t)
{apply(total_claim, 2, function(x) mean(x[x > t]))})
V[is.na(V)] <- 0 # replace NA with zeros
Vt <- colMeans(V) # mean for each client
Vt_sd <- apply(V, 2, sd) # sd
log(Vt2 )
plot(log(Vt2 ))
Vt2
Vt
knitr::opts_chunk$set(cache=TRUE,
message=FALSE, warning=FALSE,
fig.path='figs/',
cache.path = '_cache/',
fig.process = function(x) {
x2 = sub('-\\d+([.][a-z]+)$', '\\1', x)
if (file.rename(x, x2)) x2 else x
})
library(copula)
library(WVPlots)
library(gridExtra)
library(tidyverse)
library(knitr)
tinytex::install_tinytex()
knitr::opts_chunk$set(cache=TRUE,
message=FALSE, warning=FALSE,
fig.path='figs/',
cache.path = '_cache/',
fig.process = function(x) {
x2 = sub('-\\d+([.][a-z]+)$', '\\1', x)
if (file.rename(x, x2)) x2 else x
})
library(copula)
library(WVPlots)
library(gridExtra)
library(tidyverse)
library(knitr)
#tinytex::install_tinytex()
options(digits.secs = 3)
knitr::opts_chunk$set(cache=TRUE,
message=FALSE, warning=FALSE,
fig.path='figs/',
cache.path = '_cache/',
fig.process = function(x) {
x2 = sub('-\\d+([.][a-z]+)$', '\\1', x)
if (file.rename(x, x2)) x2 else x
})
library(copula)
library(WVPlots)
library(gridExtra)
library(tidyverse)
library(knitr)
tinytex::install_tinytex()
options(digits.secs = 3)
plot(seq(100,200,10), mean_boot, xlab ='threshold(t)', ylab ='P/V', type='b')
mean_boot <- apply(V3, 2, mean)
load('Vt_boot.RData')
### construct the confident intervals
mean_boot <- apply(V3, 2, mean)
se_boot <- apply(V3, 2, sd) # Bootstrap se's of the coefficients
### calculate 80% confidence intervals for V(t)
CI_low <- mean_boot + se_boot * qnorm(0.1)
CI_low[CI_low < 0] <- 0
CI_up  <- mean_boot + se_boot * qnorm(0.9)
plot(seq(100,200,10), mean_boot, xlab ='threshold(t)', ylab ='P/V', type='b')
lines(seq(100,200,10), CI_low, lty=2)
lines(seq(100,200,10), CI_up, lty=2)
lines(seq(100,200,10), Pt,type='b', col='red')
legend( x = 'topright', legend = c('Vt','Pt'), pch=1,
col = c('black', 'red'))
arrows(t, L, t, U, length=0.05, angle=90, code=3)
plot(seq(100,200,10), mean_boot, xlab ='threshold(t)', ylab ='P/V', type='b')
lines(seq(100,200,10), CI_low, lty=2)
lines(seq(100,200,10), CI_up, lty=2)
lines(seq(100,200,10), Pt,type='b', col='red')
legend( x = 'topright', legend = c('Vt','Pt'), pch=1,
col = c('black', 'red'))
arrows(seq(100,200,10), CI_low, seq(100,200,10), CI_up, length=0.05, angle=90, code=3)
plot(seq(100,200,10), mean_boot, xlab ='threshold(t)', ylab ='P/V', type='b')
lines(seq(100,200,10), Pt,type='b', col='red')
legend( x = 'topright', legend = c('Vt','Pt'), pch=1,
col = c('black', 'red'))
arrows(seq(100,200,10), CI_low, seq(100,200,10), CI_up, length=0.05, angle=90, code=3)
quantile(V3)
mean_boot
V3
apply(V3, 2, quantile)
apply(V3, 2, quantile(probs = c(0.1,0.9))
)
apply(V3, 2, function(x) quantile(x, probs = c(0.1,0.9)))
apply(V3, 2, function(x) quantile(x, probs = c(0.1,0.9)))[1,]
apply(V3, 2, function(x) quantile(x, probs = c(0.1,0.9)))[2,]
### construct the confident intervals
mean_boot <- apply(V3, 2, mean)
quantile_boot <- apply(V3, 2, function(x) quantile(x, probs = c(0.1,0.9))) # quantile
### calculate 80% confidence intervals for V(t)
CI_low <- quantile_boot[1,]
CI_up  <- quantile_boot[2,]
plot(seq(100,200,10), mean_boot, xlab ='threshold(t)', ylab ='P/V', type='b')
lines(seq(100,200,10), Pt,type='b', col='red')
legend( x = 'topright', legend = c('Vt','Pt'), pch=1,
col = c('black', 'red'))
arrows(seq(100,200,10), CI_low, seq(100,200,10), CI_up, length=0.05, angle=90, code=3)
CI_low
CI_up
CI_low  - CI_up
?arrows
plot(seq(100,200,10), mean_boot, xlab ='threshold(t)', ylab ='P/V', type='b')
lines(seq(100,200,10), Pt,type='b', col='red')
legend( x = 'topright', legend = c('Vt','Pt'), pch=1,
col = c('black', 'red'))
arrows(seq(100,200,10), CI_low, seq(100,200,10), CI_up, length=0.05, angle=90, code=4)
plot(seq(100,200,10), mean_boot, xlab ='threshold(t)', ylab ='P/V', type='b')
lines(seq(100,200,10), Pt,type='b', col='red')
legend( x = 'topright', legend = c('Vt','Pt'), pch=1,
col = c('black', 'red'))
arrows(seq(100,200,10), CI_low, seq(100,200,10), CI_up, length=0.05, angle=90, code=3)
plot(seq(100,200,10), mean_boot, xlab ='threshold(t)', ylab ='P/V', type='b')
lines(seq(100,200,10), Pt,type='b', col='red')
legend( x = 'topright', legend = c('Vt','Pt'), pch=1,
col = c('black', 'red'))
arrows(seq(100,200,10), CI_low, seq(100,200,10), CI_up, length=0.005, angle=90, code=3)
t <- seq(100,200,10)
plot(t, mean_boot, xlab ='threshold(t)', ylab ='P/V', type='b')
lines(t, Pt,type='b', col='red')
legend( x = 'topright', legend = c('Vt','Pt'), pch=1,
col = c('black', 'red'))
polygon(c(t,CI_low),c(t,CI_up),col="skyblue")
arrows(t, CI_low, t, CI_up, length=0.05, angle=90, code=3)
plot(t, mean_boot, xlab ='threshold(t)', ylab ='P/V', type='b')
lines(t, Pt,type='b', col='red')
legend( x = 'topright', legend = c('Vt','Pt'), pch=1,
col = c('black', 'red'))
polygon(c(t,CI_low),c(t,CI_up),col="skyblue")
#arrows(t, CI_low, t, CI_up, length=0.05, angle=90, code=3)
plot(t, mean_boot, xlab ='threshold(t)', ylab ='P/V', type='b')
lines(t, Pt,type='b', col='red')
legend(x = 'topright', legend = c('Vt','Pt'), pch=1,
col = c('black', 'red'))
polygon(c(t,CI_low),c(t,CI_up),col=gray(0.8))
#arrows(t, CI_low, t, CI_up, length=0.05, angle=90, code=3)
plot(t, mean_boot, xlab ='threshold(t)', ylab ='P/V', type='b')
lines(t, Pt,type='b', col='red')
legend(x = 'topright', legend = c('Vt','Pt'), pch=1,
col = c('black', 'red'))
polygon(c(t,t),c(CI_low,CI_up),col=gray(0.8))
#arrows(t, CI_low, t, CI_up, length=0.05, angle=90, code=3)
rev(t)
plot(t, mean_boot, xlab ='threshold(t)', ylab ='P/V', type='b')
lines(t, Pt,type='b', col='red')
legend(x = 'topright', legend = c('Vt','Pt'), pch=1,
col = c('black', 'red'))
polygon(c(t,rev(t),c(CI_low,CI_up),col=gray(0.8))
#arrows(t, CI_low, t, CI_up, length=0.05, angle=90, code=3)
plot(t, mean_boot, xlab ='threshold(t)', ylab ='P/V', type='b')
lines(t, Pt,type='b', col='red')
legend(x = 'topright', legend = c('Vt','Pt'), pch=1,
col = c('black', 'red'))
polygon(c(t,rev(t)),c(CI_low,CI_up),col=gray(0.8))
#arrows(t, CI_low, t, CI_up, length=0.05, angle=90, code=3)
plot(t, mean_boot, xlab ='threshold(t)', ylab ='P/V', type='b')
lines(t, Pt,type='b', col='red')
legend(x = 'topright', legend = c('Vt','Pt'), pch=1,
col = c('black', 'red'))
polygon(c(t,rev(t)),c(CI_low,rev(CI_up)),col=gray(0.3))
#arrows(t, CI_low, t, CI_up, length=0.05, angle=90, code=3)
plot(t, mean_boot, xlab ='threshold(t)', ylab ='P/V', type='b')
lines(t, Pt,type='b', col='red')
legend(x = 'topright', legend = c('Vt','Pt'), pch=1,
col = c('black', 'red'))
polygon(c(t,rev(t)),c(CI_low,rev(CI_up)),col='gray', alpha=0.1)
#arrows(t, CI_low, t, CI_up, length=0.05, angle=90, code=3)
plot(t, mean_boot, xlab ='threshold(t)', ylab ='P/V', type='b')
lines(t, Pt,type='b', col='red')
legend(x = 'topright', legend = c('Vt','Pt'), pch=1,
col = c('black', 'red'))
polygon(c(t,rev(t)),c(CI_low,rev(CI_up)),col=rgb(1, 0, 0,0.5))
#arrows(t, CI_low, t, CI_up, length=0.05, angle=90, code=3)
plot(t, mean_boot, xlab ='threshold(t)', ylab ='P/V', type='b')
lines(t, Pt,type='b', col='red')
legend(x = 'topright', legend = c('Vt','Pt'), pch=1,
col = c('black', 'red'))
polygon(c(t,rev(t)),c(CI_low,rev(CI_up)),col=rgb(100, 100, 100,0.2))
plot(t, mean_boot, xlab ='threshold(t)', ylab ='P/V', type='b')
lines(t, Pt,type='b', col='red')
legend(x = 'topright', legend = c('Vt','Pt'), pch=1,
col = c('black', 'red'))
polygon(c(t,rev(t)),c(CI_low,rev(CI_up)),col=rgb(.5, .5, .5,0.2))
#arrows(t, CI_low, t, CI_up, length=0.05, angle=90, code=3)
plot(t, mean_boot, xlab ='threshold(t)', ylab ='P/V', type='b')
lines(t, Pt,type='b', col='red')
legend(x = 'topright', legend = c('Vt','Pt'), pch=1,
col = c('black', 'red'))
polygon(c(t,rev(t)),c(CI_low,rev(CI_up)),col=rgb(.5, .5, .5,0.2))
arrows(t, CI_low, t, CI_up, length=0.05, angle=90, code=3)
plot(t, mean_boot, xlab ='threshold(t)', ylab ='P/V', type='b')
lines(t, Pt,type='b', col='red')
legend(x = 'topright', legend = c('Vt','Pt'), pch=1,
col = c('black', 'red'))
polygon(c(t,rev(t)),c(CI_low,rev(CI_up)),col=rgb(.5, .5, .5,0.2))
#arrows(t, CI_low, t, CI_up, length=0.05, angle=90, code=3)
plot(t, mean_boot, xlab ='threshold(t)', ylab ='P/V', ylim=c(0,0.05), type='b')
lines(t, Pt,type='b', col='red')
legend(x = 'topright', legend = c('Vt','Pt'), pch=1,
col = c('black', 'red'))
polygon(c(t,rev(t)),c(CI_low,rev(CI_up)),col=rgb(.5, .5, .5,0.2))
#arrows(t, CI_low, t, CI_up, length=0.05, angle=90, code=3)
plot(t, mean_boot, xlab ='threshold(t)', ylab ='P/V', ylim=c(0,0.05), type='b')
lines(t, Pt,type='b', col='red')
legend(x = 'topright', legend = c('Vt','Pt'), pch=1,
col = c('black', 'red'))
polygon(c(t,rev(t)),c(CI_low,rev(CI_up)),col=rgb(.5, .5, .5,0.2))
arrows(t, CI_low, t, CI_up, length=0.05, angle=90, code=3)
plot(t, mean_boot, xlab ='threshold(t)', ylab ='P/V', ylim=c(0,0.05), type='b')
lines(t, Pt,type='b', col='red')
legend(x = 'topright', legend = c('Vt','Pt'), pch=1,
col = c('black', 'red'))
polygon(c(t,rev(t)),c(CI_low,rev(CI_up)),col=rgb(.5, .5, .5,0.2))
arrows(t, CI_low, t, CI_up, length=0.05, angle=90, code=3)
plot(t, mean_boot, xlab ='threshold(t)', ylab ='P/V', ylim=c(0,0.05), type='l')
lines(t, Pt,type='l', col='red')
legend(x = 'topright', legend = c('Vt','Pt'), pch=1,
col = c('black', 'red'))
polygon(c(t,rev(t)),c(CI_low,rev(CI_up)),col=rgb(.5, .5, .5,0.2))
arrows(t, CI_low, t, CI_up, length=0.05, angle=90, code=3)
plot(t, mean_boot, xlab ='threshold(t)', ylab ='P/V', ylim=c(0,0.05), type='b')
lines(t, Pt,type='b', col='red')
legend(x = 'topright', legend = c('Vt','Pt'), pch=1,
col = c('black', 'red'))
polygon(c(t,rev(t)),c(CI_low,rev(CI_up)),col=rgb(.5, .5, .5,0.2))
arrows(t, CI_low, t, CI_up, length=0.05, angle=90, code=3)
knitr::opts_chunk$set(cache=TRUE,
message=FALSE, warning=FALSE,
fig.path='figs/',
cache.path = '_cache/',
fig.process = function(x) {
x2 = sub('-\\d+([.][a-z]+)$', '\\1', x)
if (file.rename(x, x2)) x2 else x
})
library(copula)
library(WVPlots)
library(gridExtra)
library(tidyverse)
library(knitr)
tinytex::install_tinytex()
options(digits.secs = 3)
### new parameters est2 for importance sampling
est2 <- est
### setting higher mu's to make the events more probable
est2$est1[1] <- est2$est1[1] + 1
est2$est2[1] <- est2$est2[1] + 1
dmycopula <- function(Y, est) {
### This function calculate the density of the copula model at Y
### Y: the observed data with X1 and X2
x1 <- Y[,1]; x2 <- Y[,2]
u1 <- plnorm(x1, meanlog = est$est1[1], sdlog = est$est1[2])
u2 <- plnorm(x2, meanlog = est$est2[1], sdlog = est$est2[2])
dlnorm(x1, est$est1[1], est$est1[2]) *
dlnorm(x2, est$est2[1], est$est2[2]) *
dCopula(cbind(u1, u2), joeCopula(est$est3))
}
simulated_data2 <- rmycopula(B, est2$est1[1], est2$est1[2],
est2$est2[1], est2$est2[2], est2$est3)
total_claim2 <- rowSums(simulated_data2)
V2 <- sapply(t, function(t) {
ind <- (total_claim2 > t) # index for the data with total claim > t
temp <- simulated_data2[ind,] # get Y for dmycopula
### importance sampling
c(total_claim2[ind] * dmycopula(temp, est)/dmycopula(temp, est2), rep(0, B - sum(ind)))  # replace claims <= t with zeros
})
Vt2 <- colMeans(V2) # mean for each client
Vt_sd2 <- apply(V2, 2, sd) # sd
knitr::opts_chunk$set(cache=TRUE,
message=FALSE, warning=FALSE,
fig.path='figs/',
cache.path = '_cache/',
fig.process = function(x) {
x2 = sub('-\\d+([.][a-z]+)$', '\\1', x)
if (file.rename(x, x2)) x2 else x
})
library(copula)
library(WVPlots)
library(gridExtra)
library(tidyverse)
library(knitr)
#tinytex::install_tinytex()
options(digits.secs = 3)
### new parameters est2 for importance sampling
est2 <- est
### setting higher mu's to make the events more probable
est2$est1[1] <- est2$est1[1] + 1
est2$est2[1] <- est2$est2[1] + 1
dmycopula <- function(Y, est) {
### This function calculate the density of the copula model at Y
### Y: the observed data with X1 and X2
x1 <- Y[,1]; x2 <- Y[,2]
u1 <- plnorm(x1, meanlog = est$est1[1], sdlog = est$est1[2])
u2 <- plnorm(x2, meanlog = est$est2[1], sdlog = est$est2[2])
dlnorm(x1, est$est1[1], est$est1[2]) *
dlnorm(x2, est$est2[1], est$est2[2]) *
dCopula(cbind(u1, u2), joeCopula(est$est3))
}
simulated_data2 <- rmycopula(B, est2$est1[1], est2$est1[2],
est2$est2[1], est2$est2[2], est2$est3)
total_claim2 <- rowSums(simulated_data2)
V2 <- sapply(t, function(t) {
ind <- (total_claim2 > t) # index for the data with total claim > t
temp <- simulated_data2[ind,] # get Y for dmycopula
### importance sampling
c(total_claim2[ind] * dmycopula(temp, est)/dmycopula(temp, est2), rep(0, B - sum(ind)))  # replace claims <= t with zeros
})
Vt2 <- colMeans(V2) # mean for each client
Vt_sd2 <- apply(V2, 2, sd) # sd
save(Vt2, Vt_sd2, file = 'Vt2.RData')
rm(V2, simulated_data2, total_claim2)
results2 <- cbind(Pt, Vt2, Vt_sd2, Pt < Vt2)
results2 <- as.data.frame(results2)
names(results2) <- c('P(t)', 'V(t)', 'Std of V(t)', 'P(t) < V(t)')
rownames(results2) <- t
kable(results2, title="P(t) and V(t) based on importance sampling", header=T)
plot(t, Vt2, xlab ='threshold(t)', ylab ='P/V', type='b')
lines(t, Pt, type='b', col='red')
legend( x = 'topright', legend = c('Vt','Pt'), pch=1,
col = c('black', 'red'))
#### Empirical bootstrap algorithm
b <- 1000 # number of bootstrap runs
set.seed(1234)
### draw samples from the observed data; using empirical bootstrapping
data_boot <- data[sample(1:k, size = b*k, replace = TRUE),]
V3 <- c()
for (i in 1:b) {
x1_boot <- data_boot[(k*(i-1)+1):(k*i),2]
x2_boot <- data_boot[(k*(i-1)+1):(k*i),3]
### using the above-mentioned estimate function to get estimated parameters
est_boot <- estimate(x1_boot,x2_boot)
### create est2 for importance sampling
est2_boot <- est_boot
est2_boot$est1[1] <- est2_boot$est1[1]+ 1
est2_boot$est2[1] <- est2_boot$est1[1]+ 1
### importance sampling
simulated_data3 <- rmycopula(B, est2_boot$est1[1], est2_boot$est1[2],
est2_boot$est2[1], est2_boot$est2[2], est2_boot$est3)
total_claim3 <- rowSums(simulated_data3)
V_new <- sapply(t, function(t) {
ind <- (total_claim3 > t) # index for the data with total claim > t
temp <- simulated_data3[ind,] # get Y for dmycopula
c(total_claim3[ind] * dmycopula(temp, est_boot)/dmycopula(temp, est2_boot),
rep(0, B - sum(ind))) } # replace claims <= t with zeros
)
V3 <- rbind(V3,colMeans(V_new))  # add a row of V(t) for each client
}
save(V3, file = 'Vt_boot.RData')
rm(V_new, simulated_data3, total_claim3)
### construct the confident intervals
mean_boot <- apply(V3, 2, mean)
quantile_boot <- apply(V3, 2, function(x) quantile(x, probs = c(0.1,0.9))) # quantile
### calculate 80% confidence intervals for V(t)
CI_low <- quantile_boot[1,]
CI_up  <- quantile_boot[2,]
plot(t, mean_boot, xlab ='threshold(t)', ylab ='P/V', ylim=c(0,0.05), type='b')
lines(t, Pt,type='b', col='red')
legend(x = 'topright', legend = c('Vt','Pt'), pch=1,
col = c('black', 'red'))
polygon(c(t,rev(t)),c(CI_low,rev(CI_up)),col=rgb(.5, .5, .5,0.2))
arrows(t, CI_low, t, CI_up, length=0.05, angle=90, code=3)
plot(t, mean_boot, xlab ='threshold(t)', ylab ='P/V', ylim=c(0,0.05), type='b')
lines(t, Pt,type='b', col='red')
legend(x = 'topright', legend = c('Vt','Pt'), pch=1,
col = c('black', 'red'))
polygon(c(t,rev(t)),c(CI_low,rev(CI_up)),
border = rgb(.5, .5, .5, 0),
col= rgb(.5, .5, .5, 0.2))
arrows(t, CI_low, t, CI_up, length=0.05, angle=90, code=3)
results3 <- cbind(Pt, CI_low, Pt < CI_low)
results3 <- as.data.frame(results3)
names(results3) <- c('P(t)', 'Lower bound of CI(V(t))', 'P(t) < V(t)_Lower')
rownames(results3) <- t
print("P(t) and the 90% confidence interval of V(t)")
kable(results3, title="P(t) and the 90% confidence interval of V(t)", header=T)
results3
time
RMSE
T
t
knitr::opts_chunk$set(cache=TRUE,
message=FALSE, warning=FALSE,
fig.path='figs/',
cache.path = '_cache/',
fig.process = function(x) {
x2 = sub('-\\d+([.][a-z]+)$', '\\1', x)
if (file.rename(x, x2)) x2 else x
})
library(copula)
library(WVPlots)
library(gridExtra)
library(tidyverse)
library(knitr)
tinytex::install_tinytex()
options(digits.secs = 3)
Pt <- 40000 * exp(-t/7)
results <- cbind(Pt, Vt, Vt_sd, Pt < Vt)
results <- as.data.frame(results)
names(results) <- c('P(t)', 'V(t)', 'Std for V(t)', 'P(t) < V(t)')
rownames(results) <- t
print("P(t) and V(t) based on 10^5 simulutation samples")
kable(results, title="P(t) and V(t) based on 10^5 simulutation samples", header=T)
Pt <- 40000 * exp(-t/7)
results <- cbind(Pt, Vt, Vt_sd, Pt < Vt)
results <- as.data.frame(results)
names(results) <- c('P(t)', 'V(t)', 'Std for V(t)', 'P(t) < V(t)')
rownames(results) <- t
print("P(t) and V(t) based on 10^5 simulutation samples")
kable(results, title="P(t) and V(t) based on 10^5 simulutation samples", header=T)
Pt <- 40000 * exp(-t/7)
results <- cbind(Pt, Vt, Vt_sd, Pt < Vt)
results <- as.data.frame(results)
names(results) <- c('P(t)', 'V(t)', 'Std for V(t)', 'P(t) < V(t)')
rownames(results) <- t
print("P(t) and V(t) based on 10^5 simulutation samples")
kable(results, title="P(t) and V(t) based on 10^5 simulutation samples", header=T)
exp(-t/7)
