---
output: 
  pdf_document:
    citation_package: natbib
    keep_tex: true
    fig_caption: true
    latex_engine: pdflatex
    template: svm-latex-ms.tex
bibliography: master.bib
header-includes:
  -  \usepackage{hyperref}
biblio-style: apsr
title: "End-term project: Advanced Statistical Computing 2020"
#thanks: "**Date: `r format(Sys.time(), '%B %d, %Y')`; **Corresponding author**: jeremydai1992@gmail.com"
author:
- name: Yizhen Dai
  affiliation: s2395479
abstract: "This report is the result of the efforts for the final exam/project of Advanced Statistical Computating. The goal of this project is to analyze the cost and the benefit of a re-insruance policy through simulation."
keywords: "Advanced Statistical Computating, Final Project, Simulation"
date: "`r format(Sys.time(), '%B %d, %Y')`"
geometry: margin=1in
fontfamily: mathpazo
fontsize: 11pt
# spacing: double
endnote: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE,
                      message=FALSE, warning=FALSE,
                      fig.path='figs/',
                      cache.path = '_cache/',
                      fig.process = function(x) {
                      x2 = sub('-\\d+([.][a-z]+)$', '\\1', x)
                      if (file.rename(x, x2)) x2 else x
                      })

library(copula)
library(WVPlots)
library(gridExtra)
library(tidyverse)
library(knitr)
#tinytex::install_tinytex()
options(digits.secs = 3)
```


# Introduction
## The goal
This project aims at solving a modeling problem faced by an insurance company - ANV. Two of ANV business lines, [Professional liability insurance](https://en.wikipedia.org/wiki/Professional_liability_insurance) (PLI) and [Workers’ compensation](https://en.wikipedia.org/wiki/Workers%27_compensation) (WC), were affected by a huge claim from one client during the last year. Therefore, ANV comes to a reinsurance company for an insurance policy. To put the problem in a mathematical format, we define the following notation:

* $X_1$: the loss incurred for PLI from ANV clients (in million euros);
* $X_2$: the loss incurred for WC from ANV clients (in million euros);
* $t$: the threshold set by the reinsurance company (in million euros). For some threshold $t = 100,110,…,200$, if the total loss incurred for PLI and WC from a certain client exceeds $t$, ANV pays the claim themselves; Otherwise, the reinsurance company pays the claim.

* $V(t)$: the expected over-threshold claims, which is equal to $E[(X_1+X_2)1(X_1+X_2>t)]$;
* $P(t)$: the price that reinsurance company asks. $P(t)$ depends on the threshold $t$ as $P(t)=40000 * e^{-t/7}$.

Our goal is to determine if ANV should buy such policy from the reinsurance company. Of course, the policy is only reasonable if the expected over-threshold claim exceeds the price: $V(t) > P(t)$. The data available is the loss incurred for PLI and WC for all the clients of ANV during last year. With limited data available, we use statistical modeling to approximate $V(t)$ and therefore determine if $V(t) > P(t)$.

## Models
```{r include=FALSE}
rm(list=ls())

data <- read.csv('insurance.csv')
x1 <- data[,2]; x2 <- data[, 3];
cor(x1, x2)
```

$X_1$ and $X_2$ are positively correlated. The correlation between $X_1$ and $X_2$ is 0.528. The linear regression line (in red) and the *LOWESS* smoother line (in blue) is shown below in Figure 1.

```{r,echo=F ,fig.cap="The relationship between X1 and X2"}
plot(x1, x2)
abline(lm(x2 ~ x1), col = "red")
lines(lowess(x1,x2), col = "blue")
```

In order to reflect the dependence in the data, we cannot simulate $X_1$ and $X_2$ separately. We use a Copula model to model the joint density $f_{X_{1}, X_{2}}$.

$$
f_{X_{1}, X_{2}}\left(x_{1}, x_{2}\right)=f_{X_{1}}\left(x_{1}\right) f_{X_{2}}\left(x_{2}\right) c\left(u_1,u_2\right)
$$

where $u_1 = F_{X_{1}}\left(x_{1}\right),u2= F_{X_{2}}\left(x_{2}\right)$, which are the marginal functions of $f_{X_{1}}$ and $f_{X_{2}}$; $c\left(u_{1}, u_{2}\right)$ is the joint integral transforms  $U_{1}=F_{X_{1}}\left(X_{1}\right)$ and $U_{2}=F_{X_{2}}\left(X_{2}\right)$.

Preliminary experiments suggested the following parametric models:

$f_{X_{1}}\left(\cdot ; \mu_{1}, \sigma_{1}\right)  \sim \textrm{Lognormal} \left(\mu_{1}, \sigma_{1}\right), \mu_{1} \in \mathbb{R}, \sigma_{1}>0$

$f_{X_{2}}\left(\cdot ; \mu_{2}, \sigma_{2}\right)  \sim \textrm{Lognormal} \left(\mu_{2}, \sigma_{2}\right), \mu_{2} \in \mathbb{R}, \sigma_{2}>0$

$c(\cdot ; \theta)  \sim \textrm{Joe}(\theta), \theta \geq 1$

where a Lognormal distribution is for a random variable whose logarithm is normally distributed; a Joe distribution is one of the most prominent bivariate Archimedean copulas in Copula models.

# Methodolgy
## Parameter Estimates
Based on the models, we have a total of five parameters to be estimated from the data: $\mu_{1},\sigma_{1},\mu_{2},\sigma_{2},\theta$. The maximum likelihood estimation is used to estimate these parameters. The idea behind is to estimate these five parameters by maximizing likelihood function. 

First, we estimate $\mu_{1},\sigma_{1},\mu_{2},\sigma_{2}$ by finding the values that can maximize the sum of log density of the observed data so the data is most probable under our model assumption $f_{X_{1}},f_{X_{2}}$. To speed up the optimization, we can use the sample mean and sample standard deviation of the observed data on the log scale.

$\hat{\mu} = \arg \max_{\mu}\left\{\ln \left\{\prod f_{X}\left(  \mu \mid x\right)\right\}\right\}$

$\hat{\sigma} = \arg \max _{\sigma}\left\{\ln \left\{\prod f_{X}\left( \sigma \mid x \right)\right\}\right\}$

Afterwards, we estimate $\theta$ using the probability integral transforms $u_1,u_2$ of the observed data $x_1,x_2$ using the cumulative distribution function of Lognormal distribution $F_{X_{1}},F_{X_{2}}$. Such transforms are not observed get pseudo-observations by plugging in the estimated $\mu_{1},\sigma_{1},\mu_{2},\sigma_{2}$. It would work if our estimated parameters are close to the true parameters.

$u_1 = F_{X_{1}}(x_1 \mid \hat{\mu_1}, \hat{\sigma_1})$

$u_2 = F_{X_{2}}(x_2 \mid \hat{\mu_2}, \hat{\sigma_2})$

$\hat{\theta} = \arg \max _ {\theta}\left\{\ln \left\{\prod_{i} c\left(\theta \mid u_1, u_2\right)\right\}\right\}$

```{r include=F}
t <- seq(100, 200, 10) # the thresholds

get_theta <- function(x1, x2, est1, est2){
  ### This function estimate theta using pseudo-observations
  
  ### x1 and x2 are the observed data
  
  ### est1 and est2 are the estimated parameters of lognormal distribution

  
  ### get u1 and u2 based on the estimated mu and sigma
  u1 <- plnorm(x1, meanlog = est1[1], sdlog = est1[2])
  u2 <- plnorm(x2, meanlog = est2[1], sdlog = est2[2])
  ### using Joe(theta)-density; evaluated at u; u is a (n x 2) matrix
  g3 <- function(theta) {
    - sum(dCopula(cbind(u1, u2), joeCopula(theta), log = T), na.rm = T)}
  ### minimize the negative log likelihood function for the copula function
  nlminb(2, g3, lower = 1 + 10^-8, upper = Inf)$par
}


estimate <- function(x1,x2){
  ### This function estimate the five parameters in our Copula model
  
  ### x1 and x2 are the observed data

  meanlog <- sapply(list(log(x1),log(x2)), mean)
  sdlog <- sapply(list(log(x1),log(x2)), sd)
  
  ### negative log likelihood function to be minimized
  g1 <- function(beta) {
    - sum(dlnorm(x1, meanlog = beta[1], sdlog = beta[2], log = T), na.rm = T)}
  g2 <- function(beta) {
    - sum(dlnorm(x2, meanlog = beta[1], sdlog = beta[2], log = T), na.rm = T)}
  
  ### Compute estimates for x1 by maximum likelihood method
  est1 <- nlminb(c(meanlog[1],sdlog[1]), g1, lower = c(-Inf, 0), upper = Inf)$par
  names(est1) <- c('mu1','sigma1')
  
  ### Compute estimates for x2 by maximum likelihood method
  est2 <- nlminb(c(meanlog[2],sdlog[2]), g2, lower = c(-Inf, 0), upper = Inf)$par
  names(est2) <- c('mu2','sigma2')
  
  ### Estimate the parameters of a Joe copula model for (U1,U2).
  est3 <- get_theta(x1, x2, est1, est2)
  
  return(list('est1' = est1, 'est2' = est2, 'est3' = est3))
}
```


```{r, echo=F}
est <- estimate(x1,x2)
```

The resulted estimated parameters are $\hat{\mu_{1}}= 1.982,\hat{\sigma_{1}}=0.513,\hat{\mu_{2}}=2.997,\hat{\sigma_{2}}=0.311,\hat{\theta}=1.608$.

## Simulated data
We get the parameter estimates, then we can simulate enough data to get the average cost $V(t)$ over the years. 

To generate enough data, we need a function that simulates from the joint model for $(X_1,X_2)$ for a given set of parameters $(\mu_{1}, \sigma_{1},\mu_{2},\sigma_{2},\theta)$. We firstly simulate $u_1, u_2$ using the density function of Joe. Then we get $x_1, x_2$ based on Lognormal distributions.

```{r include=F}
rmycopula <- function(n, mu1, sd1, mu2, sd2, theta){
  ### This function simulates n samples from a Joe copula with estimated parameters 
  
  ### use rCopula(n, joeCopula(theta)) to simulate u1 and u2
  u <- rCopula(n, joeCopula(theta)) # u is a (n x 2) matrix
  x1 <- qlnorm(u[,1], mu1, sd1)
  x2 <- qlnorm(u[,2], mu2, sd2)
  as.data.frame(cbind(x1,x2))
}
```

To evaluate our estimated model, we can compare the simulated data with the observed data. They are shown in Figure 2 and Figure 3.

```{r ,echo=F, fig.cap="Observed Data"}
set.seed(1234)
k <- length(data$ID) # number of data points in the dataset
simulated_data <- rmycopula(k, est$est1[1], est$est1[2], est$est2[1], est$est2[2], est$est3)

ScatterHist(data, 'PLI', 'WC', title='Observed Data', contour = T)
```

```{r ,echo=F, fig.cap="Simulated Data"}
ScatterHist(simulated_data, 'x1', 'x2', title='Simulated Data', contour = T)
```

We can see the marginal and joint distributions of observed data and simulated data are similar, which is a good sign of our implementation.

We can also check change in data distributions when we tune the parameters using our simulation function. Currently $\hat{\mu_{1}},= 1.982, \hat{\sigma_{1}}=0.513,\hat{\mu_{2}}=2.997,\hat{\sigma_{2}}=0.311,\hat{\theta}=1.608$.

First, we increase $\theta$ to 4. The results are shown in Figure 4. We can see with a bigger theta, the data are more correlated with smaller deviation from the linear regression line.

```{r ,echo=F ,fig.cap="theta = 4"}
set.seed(1234)
simulated_data <- rmycopula(k, est$est1[1], est$est1[2], est$est2[1], est$est2[2], 4)

ScatterHist(simulated_data, 'x1', 'x2', title='Simulated Data', contour = T)
```


Next, we check the distribution with bigger $\mu_1=3, \mu_2=4$. The results are shown in Figure 5. We can see when we increase $\mu_1,\mu_2$, the center of the contour also shifts in the same direction.

```{r ,echo=F ,fig.cap="mu1 = 3, mu2 = 4"}
set.seed(1234)
simulated_data <- rmycopula(k, 3, est$est1[2], 4, est$est2[2], est$est3)

ScatterHist(simulated_data, 'x1', 'x2', title='mu1 = 3, mu2 = 4', contour = T)
```


Next, we check the distribution with bigger standard deviation of Lognormal distribution: $\sigma_1 =\sigma_2 = 0.8$. The results are shown in Figure 6. We can see that the data is more dispersed, which is same as expected. We can also observe that a small change in $\sigma_1, \sigma_2$ has a big influence on the extreme values. This is partly because the $\sigma$ is on the log scale.

```{r,echo=F ,fig.cap="sigma1 = sigma2 = 0.8"}
set.seed(1234)
simulated_data <- rmycopula(k, est$est1[1], .8, est$est2[1], .8, est$est3)

ScatterHist(simulated_data, 'x1', 'x2', title='sigma1 = sigma2 = 0.8', contour = T)
```


# Simulation Study
## Acuracy evaluation

To evaluate how accurate our parameter estimation can be, we conduct a simulation study assuming the following values for true parameters: $\mu_1=1,\sigma_1=2,\mu_2=3,\sigma_2=0.5,\theta=2$.

We compute the RMSE for 100 simulation runs of 200,500 and 1000 data points. 

```{r, include=F, eval=F}
fit_n_times <- function(n){
  ### This function estimates the parameters for 100 times and returns the RMSE
  
  ### n: the number of data points in each simulation run
  
  ### simulate
  sim_data <- lapply(1:100,function(x) rmycopula(n, 1, 2, 3, 0.5, 2))
  results <- c()
  for (i in 1:100){
    results <- cbind(results, 
                     unlist(estimate(sim_data[[i]]$x1, sim_data[[i]]$x2)))
  }
  ### calculate RMSE
  (results - c(1, 2, 3, 0.5, 2))^2 %>%   # estimated minus true values
    rowMeans(.) %>% 
    sqrt(.)
}

### initiate time and RMSE
time <- numeric(3)
RMSE <- matrix(0, 3, 5) 

### conduct simulation for n = 200,500 and 1000
i <- 1
for (n in c(2,5,10) * 100) {
  time[i] <- system.time(RMSE[i,] <- fit_n_times(n))/100 # average computing time
  i <- i + 1
}

### update format for plotting
time <- data.frame(number_of_runs=c(2,5,10) * 100, time=time)
```

```{r ,eval=F, include=F}
save(time, RMSE, file = 'fit_n_times.RData')
```

```{r ,echo=F}
load('fit_n_times.RData')
```


```{r ,echo=F ,fig.cap="Average computing time vs number of runs"}
### plot time
plot(time, xlab = 'number of runs', type ='b')
```

```{r ,echo=F ,fig.cap="RMSE vs number of runs"}
### plot RMSE
RMSE <- as.data.frame(RMSE)
names(RMSE) <- c('mu1', 'sigma1', 'mu2', 'sigma2', 'theta')
plot(NULL, xlab = 'number of runs', ylab = 'RMSE', 
     xlim=c(0, 10^3), ylim=c(min(RMSE), max(RMSE))) 
number_of_runs <- c(2,5,10) * 100
lines(number_of_runs, type ='b', RMSE$mu1, col='red')
lines(number_of_runs, type ='b', RMSE$sigma1, col='orange')
lines(number_of_runs, type ='b', RMSE$mu2, col='black')
lines(number_of_runs, type ='b', RMSE$sigma2, col='blue')
lines(number_of_runs, type ='b', RMSE$theta, col='green')
legend( x = 'topright', legend = names(RMSE), pch=1, 
        col = c('red', 'orange', 'black', 'blue', 'green'))
```

The average computing time and RMSE are shown in Figure 7 and Figure 8. We can see that required computation time increase linearly with the number of runs. RMSE gets smaller with more simulation runs. $\theta$ is the hardest parameter to estimate with largest RMSE.  $\mu_1, \sigma_1$ have higher RMSE than $\mu_2, \sigma_2$ separately. This is probably due to a lower standard deviation for $f_{X_2}$ (0.5 compared to 2 on the log scale). The distribution for $f_{X_1}$ is more dispersed, leading to a larger RMSE for $\mu_1, \sigma_1$.


## Working on the real data: V(t)
Having a feeling of the simulation models, we fit all the five parameters to the observed data. We compute the expected payout of the reinsurance $V(t)$ using parametric Monte Carlo simulation (based on the estimated parameters). We use $10^5$ Monte Carlo samples and calculate expected $V(t), t=100,110,…,200$, as the average over the simulation runs.


```{r , include=F ,eval = F}
B <- 10^5 # number of simulation runs

### simulated B data points to save time
simulated_data <- rmycopula(B, est$est1[1], est$est1[2], est$est2[1], est$est2[2], est$est3)
total_claim <- rowSums(simulated_data) # x1 + x2

### replace claims <= t with zeros
V <- sapply(t, function(t) 
  c(total_claim[total_claim > t], rep(0,sum(total_claim <= t))))

Vt <- colMeans(V) # mean for each client
Vt_sd <- apply(V, 2, sd) # sd
```

```{r ,include=F ,eval = F}
save(Vt, Vt_sd, file = 'Vt.RData')
rm(V, simulated_data, total_claim)
```

```{r ,echo=F}
load('Vt.RData')
```

The plot of $V(t)$ for $t=100,110,…,200$ based on $10^5$ Monte Carlo samples is shown in Figure 9.

```{r ,echo=F ,size='small', cache=FALSE, results="asis", message=F, warning=F}
Pt <- 40000 * exp(0 - t / 7) 
results <- cbind(Pt, Vt, Vt_sd, Pt < Vt)
results <- as.data.frame(results)
names(results) <- c('P(t)', 'V(t)', 'Std for V(t)', 'P(t) < V(t)')
rownames(results) <- t

print("P(t) and V(t) based on 10^5 simulutation samples")
kable(results, title="P(t) and V(t) based on 10^5 simulutation samples", header=T)
```

```{r ,echo=F, fig.cap="P(t) vs t"}
plot(t, Vt, type='b', xlab = 'threshold(t)', ylab = 'P/V')
lines(t, Pt, type='b', col='red')
legend( x = 'topright', legend = c('Vt','Pt'), pch=1, 
        col = c('black', 'red'))
```

Based on the results, buying the reinsurance policy seems a good idea when $t\leq120$ since the policy price ($P(t)$)is smaller than the potential cost of not buying the policy ($V(t)$). However, the standard deviation is high for $V(t)$. This is because the claim exceeding $t$ is a highly improbable event. 

## Importance sampling
To handle the highly improbable events, we must get more samples of claims exceeding $t$. Therefore, importance sampling comes to play. It is implemented by sampling from an alternative distribution that has higher densities in the region of our interest. In our case, the alternative density function is the Copula function with new $u_{1}^{\textrm{new}},u_{2}^{\textrm{new}}$. $u_{1}^{\textrm{new}} = \hat{u_1} + 1 ,u_{2}^{\textrm{new}}= \hat{u_2}+1$. We adjust our estimate using an adjustment factor, which is the ratio of the alternative distribution density and the original distribution density.

```{r, eval=F ,include=F}
### new parameters est2 for importance sampling
est2 <- est
### setting higher mu's to make the events more probable
est2$est1[1] <- est2$est1[1] + 1
est2$est2[1] <- est2$est2[1] + 1


dmycopula <- function(Y, est) {
  ### This function calculate the density of the copula model at Y
  ### Y: the observed data with X1 and X2
  x1 <- Y[,1]; x2 <- Y[,2]
  u1 <- plnorm(x1, meanlog = est$est1[1], sdlog = est$est1[2])
  u2 <- plnorm(x2, meanlog = est$est2[1], sdlog = est$est2[2])
  dlnorm(x1, est$est1[1], est$est1[2]) * 
    dlnorm(x2, est$est2[1], est$est2[2]) *
    dCopula(cbind(u1, u2), joeCopula(est$est3)) 
}

simulated_data2 <- rmycopula(B, est2$est1[1], est2$est1[2], 
               est2$est2[1], est2$est2[2], est2$est3)
total_claim2 <- rowSums(simulated_data2) 

V2 <- sapply(t, function(t) {
  ind <- (total_claim2 > t) # index for the data with total claim > t
  temp <- simulated_data2[ind,] # get Y for dmycopula
  ### importance sampling
  c(total_claim2[ind] * dmycopula(temp, est)/dmycopula(temp, est2), rep(0, B - sum(ind)))  # replace claims <= t with zeros
  })
  
Vt2 <- colMeans(V2) # mean for each client
Vt_sd2 <- apply(V2, 2, sd) # sd

```


```{r, include=F ,eval = F}
save(Vt2, Vt_sd2, file = 'Vt2.RData')
rm(V2, simulated_data2, total_claim2)
```

```{r ,echo=F}
load('Vt2.RData')
```

The plot of $V(t)$ for $t=100,110,…,200$ based on $10^5$ Monte Carlo samples and importance sampling is shown in Figure 10. The $V(t)$ based on importance sampling have values when $t>120$. Based on the results, the company should buy the reinsurance policy regardless of the value of $t$ since $P(t) < V(t)$.


```{r ,echo=F, size='small', cache=FALSE, results="asis", message=F, warning=F}
results2 <- cbind(Pt, Vt2, Vt_sd2, Pt < Vt2)
results2 <- as.data.frame(results2)
names(results2) <- c('P(t)', 'V(t)', 'Std of V(t)', 'P(t) < V(t)')
rownames(results2) <- t

print("P(t) and V(t) based on importance sampling")
kable(results2, title="P(t) and V(t) based on importance sampling", header=T)
```

```{r ,echo=F, fig.cap="P(t) and V(t) vs t  based on importance sampling"}
plot(t, Vt2, xlab ='threshold(t)', ylab ='P/V', type='b')
lines(t, Pt, type='b', col='red')
legend( x = 'topright', legend = c('Vt','Pt'), pch=1, 
        col = c('black', 'red'))
```


## Confidence Intervals
Since we are not sure how confident we are about our estimates, we can to conduct bootstrapping to get confidence intervals of $V(t)$. The codes doing a bootstrap method (1000 times) to compute 80% confidence intervals for $V(t)$ are shown below. After getting 1000 results, we can calculate the confidence intervals by getting the 10th and 90th percentiles. The Confidence Intervals for $V(t)$ are shown in Figure 11.

```{r , eval=F, include=F}
#### Empirical bootstrap algorithm
b <- 1000 # number of bootstrap runs
set.seed(1234)

### draw samples from the observed data; using empirical bootstrapping
data_boot <- data[sample(1:k, size = b*k, replace = TRUE),] 

V3 <- c()
for (i in 1:b) { 
  x1_boot <- data_boot[(k*(i-1)+1):(k*i),2]
  x2_boot <- data_boot[(k*(i-1)+1):(k*i),3]
  ### using the above-mentioned estimate function to get estimated parameters
  est_boot <- estimate(x1_boot,x2_boot)
  ### create est2 for importance sampling
  est2_boot <- est_boot
  est2_boot$est1[1] <- est2_boot$est1[1]+ 1 
  est2_boot$est2[1] <- est2_boot$est1[1]+ 1
  

  ### importance sampling
  simulated_data3 <- rmycopula(B, est2_boot$est1[1], est2_boot$est1[2], 
               est2_boot$est2[1], est2_boot$est2[2], est2_boot$est3)
  total_claim3 <- rowSums(simulated_data3) 

  V_new <- sapply(t, function(t) {
    ind <- (total_claim3 > t) # index for the data with total claim > t
    temp <- simulated_data3[ind,] # get Y for dmycopula
    c(total_claim3[ind] * dmycopula(temp, est_boot)/dmycopula(temp, est2_boot), 
      rep(0, B - sum(ind))) } # replace claims <= t with zeros
    )
  
  V3 <- rbind(V3,colMeans(V_new))  # add a row of V(t) for each client
}
```


```{r ,include=F, eval = F}
save(V3, file = 'Vt_boot.RData')
rm(V_new, simulated_data3, total_claim3)
```

```{r ,echo=F}
load('Vt_boot.RData')
```
 
```{r, include=F}
### construct the confident intervals
mean_boot <- apply(V3, 2, mean)
quantile_boot <- apply(V3, 2, function(x) quantile(x, probs = c(0.1,0.9))) # quantile
### calculate 80% confidence intervals for V(t)
CI_low <- quantile_boot[1,]
CI_up  <- quantile_boot[2,]
```

```{r ,echo=F, size='small', cache=FALSE, results="asis", message=F, warning=F}
results3 <- cbind(Pt, CI_low, Pt < CI_low)
results3 <- as.data.frame(results3)
names(results3) <- c('P(t)', 'Lower bound of CI(V(t))', 'P(t) < V(t)_Lower')
rownames(results3) <- t

print("P(t) and the 90% confidence interval of V(t)")
kable(results3, title="P(t) and the 90% confidence interval of V(t)", header=T)
```

```{r ,echo=F, warning=F, fig.cap= '90% Confidence Interval P(t) in dashed line and V(t)' }
plot(t, mean_boot, xlab ='threshold(t)', ylab ='P/V', ylim=c(0,0.05), type='b')
lines(t, Pt,type='b', col='red')
legend(x = 'topright', legend = c('Vt','Pt'), pch=1, 
        col = c('black', 'red'))
polygon(c(t,rev(t)),c(CI_low,rev(CI_up)),
        border = rgb(.5, .5, .5, 0),
        col= rgb(.5, .5, .5, 0.2))
arrows(t, CI_low, t, CI_up, length=0.05, angle=90, code=3)
```

# Results
The implementation does account for Monte Carlo approximation error since we are doing bootstrapping. However, it does not account for estimation error since we are relying on the existing data (648 data points). 

As seen from the Figure 11, there is huge uncertainty in the values of $P$ when $t$ is 100. We cannot draw a conclusion that $V$ will be smaller than $P$ on a 90% confidence level when $t$ is 100. However, when $t\geq110$, the $P$ is smaller than the lower bound of the confidence interval $V$, indicating that the company ANV should buy the re-insurance.

