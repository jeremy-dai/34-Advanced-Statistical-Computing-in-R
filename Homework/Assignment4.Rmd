---
title: "Assignment Week 4"
output: html_document
---

### Name: Yizhen Dai

### ULCN: S2395479

```{r label = preps, echo=FALSE}
rm(list=ls()) #clears your workspace
```

### Exercise 1

We are interested in the *inverse* of the function $f(x)=x\log(x)$. Unfortunately this inverse has no closed-form solution, so we'll need to resort to numerical methods.

**(a)** Write a function that implements the inverse $f^{-1}$ of $f$ using the bisection method. It only has to work for the part of the function where $x\ge e$, where $e=2.71828\ldots$ To get suitable starting values, you can use the fact that if $x\log(x)=y$, then $x\in(e,y]$. Test your function by checking that the calculation of $f^{-1}(f(5))$ yields a result (close to) $5$. How many iterations are needed to obtain an accuracy of $10^{-6}$ in this test?

$ g(x)= x\log(x) - y $
```{r}
f <- function(x) x * log(x)

invf <- function(f, y, eps) { 
  k <- 0 # iteration
  a <- exp(1)
  b <- y
  while (b - a > eps) {
  mid <- (a + b) / 2
  if ((f(a) - y) * (f(mid)-y) < 0) b <- mid else a <- mid # for g(x)
  k <- k + 1
  }
  return(list('The solution'= mid, 'Number of iteration'= k))
}

invf(f, f(5), 10^-6)
```


**(b)** Implement the method of Newton-Raphson and test it for this same problem, i.e. to calculate the inverse of $f$ at $f(5)$. (The derivative of $f$ is $f'(x)=\log(x)+1$.) Choose starting value 10. How many iterations are needed to obtain accuracy $10^{-6}$?

Be careful! We are considering the accuracy of the estimate of $x$, so we want to know after how many iterations $x$ is sufficiently close to $5$, *not* after how many iterations $f(x)$ is sufficiently close to $f(5)$.



```{r}
invf2 <- function(f, y, eps) { 
  k <- 1 # iteration
  a <- exp(1)
  b <- a - (f(a)-y)/(log(a)+1)  # for g(x)
  while (abs(b - a) > eps) {
  a <- b
  b <- a - (f(a)-y)/(log(a)+1)  # for g(x)
  k <- k + 1
  }
  return(list('The solution'= b, 'Number of iteration'= k))
}

invf2(f, f(5), 10^-6)
```

The Newton-Raphson method converges much quickly than the bisection method.

Exercise 2
--------------
Look at the dataset `swiss` that's built into R. We wish to predict the column `Infant.Mortality` based on the other columns using ordinary least squares (OLS). The following code uses the machinery built into R to fit the parameters.

```{r}
attach(swiss)
model <- lm(Infant.Mortality ~ Fertility + Agriculture + Examination + Education + Catholic)
model$coefficients
detach(swiss)
```

The model predicts `Infant.Mortality` of each row as the inner product of the values in the other columns with the found coefficients, where the "intercept" coefficient applies to an imaginary row whose value is 1 everywhere.

We can use techniques from linear algebra to find the OLS solution ourselves:

**(a)** Define a matrix or data frame `X` whose first column is all ones, and whose subsequent columns are the columns for all the predictor variables in `swiss`. (That is, all columns from `swiss` except for `Infant.Mortality`.) Also define a vector or data frame `Y` that contains the `Infant.Mortality` column from the data set. Then fit the OLS coefficients $\hat\beta$ by hand, by using the formula

$$\hat\beta=(X^TX)^{-1}X^TY.$$

Check that the coefficients you find match the ones from the linear model.

*Hint: `solve(A)` computes the inverse of a matrix A. For an overview of matrix operations see https://www.statmethods.net/advstats/matrix.html*

```{r}
### OLS using matrix computation
data(swiss)
X <- as.matrix(cbind(int = 1, swiss[,-6]))
y <- as.matrix(swiss[6])

beta <- solve(t(X) %*% X)  %*% t(X) %*% y
beta

```

The coefficients are the same.

**(b)** Instead of using the algebraic solution, now find the OLS coefficients by minimising the mean of squared errors (MSE) with respect to the coefficients $\beta$ using the built-in optimisation method `nlm`. Again check that the coefficients match what we found before. Note: the squared error for a row $i$ is the squared difference between $Y_i$ and the inner product between $X_i$ and $\beta$. The mean squared error is the average over all rows.

```{r}
### OLS using nlm()
g <- function(beta) sum((y - X%*%beta)^2)
results1 <- nlm(g, rep(0,6), gradtol = 1e-7) # for higher precision
results1 
```

```{r}
results1$estimate - beta
```

The coefficients are the same.

**(c)** Optimisation may have seemed overkill here, but it grants additional flexibility, because we can now easily change what function we are optimising. Instead of minimising the MSE, we can for example implement Lasso regression by adding L1-regularisation: find the coefficients that minimizing the function $\text{MSE}(\beta)+\lambda\|\beta\|_1$, where $\|\beta\|_1$ is the L1-norm: the sum of the absolute values of the coefficients. Use $\lambda=0.02$.

```{r}
### L1 regularization
### lambda = 0.02; intercept is not penalized
g <- function(beta) sum((y - X%*%beta)^2) + sum(0.02*abs(beta[2:6])) 
results2 <- nlm(g, rep(0,6), gradtol = 1e-7)
results2 
```
We can see the target function ends bigger with regularization.

```{r}
results1$estimate - results2$estimate 
```
There is no big change in parameter values.